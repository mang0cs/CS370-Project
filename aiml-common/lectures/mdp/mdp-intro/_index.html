
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction to MDP &#8212; Introduction to Artificial Intelligence</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/mdp/mdp-intro/_index.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Bellman Expectation Backup" href="../bellman-expectation-backup/_index.html" />
    <link rel="prev" title="Markov Decision Processes" href="../_index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Artificial Intelligence</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../syllabus/_index.html">
   Syllabus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to AI
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/course-introduction/_index.html">
   Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/data-science-360/_index.html">
   Data Science 360
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/systems-approach/_index.html">
   The four approaches towards AI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/agents/_index.html">
   Agent-Environment Interface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ai-intro/pipelines/_index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../uber-ml-arch-case-study/_index.html">
     A Case Study of an ML Architecture - Uber
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ai-intro/pipelines/02_end_to_end_machine_learning_project.html">
     Setup
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Learning Problem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../learning-problem/_index.html">
   The Learning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../regression/linear-regression/linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/sgd/_index.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../entropy/_index.html">
   Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/maximum-likelihood/_index.html">
   Maximum Likelihood (ML) Estimation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../pgm/bayesian-inference/_index.html">
   Bayesian Inference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../pgm/bayesian-coin/_index.html">
     Bayesian Coin Flipping
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/classification-intro/_index.html">
   Introduction to Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/logistic-regression/_index.html">
   Logistic Regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/perceptron/_index.html">
   The Neuron (Perceptron)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/dnn-intro/_index.html">
   Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-intro/_index.html">
   Introduction to Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-dnn/_index.html">
   Backpropagation in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-dnn-exercises/_index.html">
   Backpropagation DNN exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/fashion-mnist-case-study.html">
   Fashion MNIST Case Study
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/regularization/_index.html">
   Regularization in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/regularization/regularization-workshop-1.html">
   Regularization Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Convolutional Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-intro/_index.html">
   Introduction to Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-layers/_index.html">
   CNN Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-example-architectures/_index.html">
   CNN Example Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-example-architectures/using_convnets_with_small_datasets.html">
   Using convnets with small datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-explainers/_index.html">
   CNN Explainers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Transfer Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../transfer-learning/transfer-learning-introduction.html">
   Introduction to Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../transfer-learning/transfer_learning_tutorial.html">
   Transfer Learning for Computer Vision Tutorial
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Scene Understanding
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/scene-understanding-intro/index.html">
   Introduction to Scene Understanding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/feature-extraction-resnet/index.html">
   Feature Extraction via Residual Networks
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../scene-understanding/object-detection/object-detection-intro/index.html">
   Object Detection
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/object-detection/detection-metrics/index.html">
     Object Detection and Semantic Segmentation Metrics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/object-detection/rcnn-object-detection/index.html">
     Region-CNN (RCNN) Object Detection
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/object-detection/faster-rcnn-object-detection/index.html">
     Fast and Faster RCNN Object Detection
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../scene-understanding/semantic-segmentation/index.html">
   Semantic Segmentation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/semantic-segmentation/maskrcnn-semantic-segmentation/index.html">
     Mask R-CNN Semantic Segmentation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/semantic-segmentation/maskrcnn-semantic-segmentation/demo.html">
     Mask R-CNN Demo
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/semantic-segmentation/maskrcnn-semantic-segmentation/inspect_data.html">
     Mask R-CNN - Inspect Training Data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/semantic-segmentation/maskrcnn-semantic-segmentation/inspect_model.html">
     Mask R-CNN - Inspect Trained Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/semantic-segmentation/maskrcnn-semantic-segmentation/inspect_weights.html">
     Mask R-CNN - Inspect Weights of a Trained Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../scene-understanding/semantic-segmentation/maskrcnn-semantic-segmentation/detectron2_tutorial.html">
     Detectron2 Beginner’s Tutorial
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probabilistic Reasoning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/pgm-intro/_index.html">
   Introduction to Probabilistic Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/recursive-state-estimation/_index.html">
   Recursive State Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/bayesian-filter-workshop/_index.html">
   Bayesian Filter Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/hmm-localization/_index.html">
   Localization and Tracking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/kalman-workshop/_index.html">
   Kalman Filter Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Logical Reasoning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../logical-reasoning/automated-reasoning/_index.html">
   Automated Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../logical-reasoning/propositional-logic/_index.html">
   World Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../logical-reasoning/logical-agents/_index.html">
   Logical Agents
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Planning without Interactions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/_index.html">
   Planning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../autonomous-cars/_index.html">
   Autonomous Agents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../autonomous-cars/imitation-learning/_index.html">
   Imitation Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/classical-planning/_index.html">
   Classical Planning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/search/_index.html">
   Planning with Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/search/forward-search/_index.html">
   Forward Search Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/search/a-star/_index.html">
   The A* Algorithm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Processes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../_index.html">
   Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Introduction to MDP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bellman-expectation-backup/_index.html">
   Bellman Expectation Backup
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../bellman-optimality-backup/_index.html">
   MDP Dynamic Programming Algorithms
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../mdp-dp-algorithms/policy-iteration/_index.html">
     Policy Iteration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mdp-dp-algorithms/policy-evaluation/_index.html">
     Policy Evaluation (Prediction)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mdp-dp-algorithms/policy-improvement/_index.html">
     Policy Improvement (Control)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mdp-dp-algorithms/value-iteration/_index.html">
     Value Iteration
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../mdp-lab/recycling-robot/_index.html">
     Finding the optimal policy of a recycling robot.
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/_index.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/prediction/monte-carlo.html">
   Monte-Carlo Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/prediction/temporal-difference.html">
   Temporal Difference (TD) Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/control/_index.html">
   Model-free Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/generalized-policy-iteration/_index.html">
   Generalized Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/control/sarsa/_index.html">
   The SARSA Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../reinforcement-learning/reinforce/_index.html">
   The REINFORCE Algorithm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sequences and RNNs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/introduction/_index.html">
   Introduction to Recurrent Neural Networks (RNN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/simple-rnn/_index.html">
   Simple RNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/lstm/_index.html">
   The Long Short-Term Memory (LSTM) Cell Architecture
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/15_processing_sequences_using_rnns_and_cnns.html">
   Processing Sequences Using RNN
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nlp-intro/_index.html">
   Introduction to NLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/word2vec/_index.html">
   Word2Vec Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/word2vec/word2vec-workshop.html">
   Word2Vec Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/language-models/_index.html">
   RNN Language Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/language-models/simple-rnn-language-model.html">
   Simple RNN Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/language-models/cnn-language-model.html">
   CNN Language Model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nmt/_index.html">
   Neural Machine Translation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nmt-metrics/_index.html">
   NMT Metrics - BLEU
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/attention/_index.html">
   Attention in NMT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/transformers/_index.html">
   Transformers Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ml-math/_index.html">
   Math for ML
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-math/probability/_index.html">
     Probability Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../linear-algebra/_index.html">
     Linear Algebra for Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-math/calculus/_index.html">
     Calculus
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../resources/environment/_index.html">
   Your Programming Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../python/_index.html">
   Learn Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../runbook.html">
   Executable Content Runbook
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pantelis/artificial-intelligence"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pantelis/artificial-intelligence/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/mdp/mdp-intro/_index.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../../_sources/aiml-common/lectures/mdp/mdp-intro/_index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-elements-of-the-agent-environment-interface">
   The elements of the Agent - Environment interface
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mdp-loop">
     MDP Loop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-transition">
     State transition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reward-function-and-returns">
     Reward function and Returns
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#returns">
       Returns
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-function">
     Policy function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#value-functions">
   Value Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-value">
     State value
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#action-value">
     Action value
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Introduction to MDP</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-elements-of-the-agent-environment-interface">
   The elements of the Agent - Environment interface
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mdp-loop">
     MDP Loop
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-transition">
     State transition
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reward-function-and-returns">
     Reward function and Returns
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#returns">
       Returns
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-function">
     Policy function
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#value-functions">
   Value Functions
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#state-value">
     State value
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#action-value">
     Action value
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="introduction-to-mdp">
<h1>Introduction to MDP<a class="headerlink" href="#introduction-to-mdp" title="Permalink to this headline">#</a></h1>
<section id="the-elements-of-the-agent-environment-interface">
<h2>The elements of the Agent - Environment interface<a class="headerlink" href="#the-elements-of-the-agent-environment-interface" title="Permalink to this headline">#</a></h2>
<p>We start by reviewing the agent-environment interface with this evolved notation and provide additional definitions that will help in grasping the concepts behind DRL. We treat MDP analytically effectively deriving the four Bellman equations.</p>
<p><img alt="agent-env-interface" src="../../../../_images/agent-env-interface.png" />
<em>Agent-Environment Interface</em></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The following table summarizes the notation and contains useful definitions that we will use to describe required concepts later.  With capital letters we denote the random variables involved and with small letters their specific realizations (values) - for example <span class="math notranslate nohighlight">\(S_t\)</span> is the random state variable and <span class="math notranslate nohighlight">\(s_t\)</span> is the actual state at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p><strong>Symbol</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(S_t\)</span></p></td>
<td><p>environment state at time step <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> the finite set of states</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(A_t\)</span></p></td>
<td><p>agent action at time step <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span> the finite set of actions</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(R_{t+1} \in \mathbb{R}\)</span></p></td>
<td><p>numerical reward sent by the environment after taking action <span class="math notranslate nohighlight">\(A_t\)</span> and transition to next state <span class="math notranslate nohighlight">\(S_{t+1}=s'\)</span></p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(t\)</span></p></td>
<td><p>time step index associated with each experience that is defined as the tuple (<span class="math notranslate nohighlight">\(S_t, A_t, R_{t+1}\)</span>).</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(T\)</span></p></td>
<td><p>maximum time step beyond which the interaction terminates</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><em>episode</em></p></td>
<td><p>the time horizon from <span class="math notranslate nohighlight">\(t=0\)</span> to <span class="math notranslate nohighlight">\(T-1\)</span></p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(\tau\)</span></p></td>
<td><p><em>trajectory</em> - the sequence of experiences over an episode</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(G_t\)</span></p></td>
<td><p><em>return</em> - the total discounted rewards from time step <span class="math notranslate nohighlight">\(t\)</span> - it will be qualified shortly.</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p><span class="math notranslate nohighlight">\(\gamma\)</span></p></td>
<td><p>the discount factor <span class="math notranslate nohighlight">\(\gamma \in [0,1]\)</span> embedded into the return <span class="math notranslate nohighlight">\(G_t\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>In fully observed MDP problems, the agent <em>perceives fully</em> the environment state <span class="math notranslate nohighlight">\(S_t\)</span>  - you can assume that there is a bank of sensors but they are ideal. In other words the agent knows which state the environment is in, perfectly.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Note that Markov processes are sometimes erroneously called <em>memoryless</em> but in any MDP above we can incorporate memory aka dependence in more than one state over time by cleverly defining the state <span class="math notranslate nohighlight">\(S_t\)</span> as a container of a number of states. For example, <span class="math notranslate nohighlight">\(S_t = \left[ S_t=s, S_{t-1} = s^\prime \right]\)</span> can still define an Markov transition using <span class="math notranslate nohighlight">\(S\)</span> states. The transition model</p>
<p>$<span class="math notranslate nohighlight">\(p(S_t | S_{t-1}) = p(s_t, s_{t-1} | s_{t-1}, s_{t-2}) = p(s_t|s_{t-1}, s_{t-2})\)</span>$ is called the 2nd-order Markov chain.</p>
<p>DeepMind’s Q-learning algorithm playing pac-man converts the non-MDP problem to MDP by accumulating four frames instead of one. With a single frame the problem was not MDP since the state of all players could not be known - with a single frame the pacman could not know if the monster was moving towards it or nor for example. With a number of frames we get to know all the information needed to survive.</p>
</div>
<section id="mdp-loop">
<h3>MDP Loop<a class="headerlink" href="#mdp-loop" title="Permalink to this headline">#</a></h3>
<p>We define a Markov Decision Process as the 5-tuple <span class="math notranslate nohighlight">\(\mathcal M = &lt;\mathcal S, \mathcal P, \mathcal R, \mathcal A, \gamma&gt;\)</span> that produces a sequence of experiences <span class="math notranslate nohighlight">\((S_t, A_t, R_{t+1}), (S_{t+1}, A_{t+1}, R_{t+2}), ...\)</span>.  The MDP (event) loop is shown below:</p>
<p><img alt="mdp-loop" src="../../../../_images/mdp-loop.png" />
<em>This generic interface between the agent and the environment captures many problems outside of pure MDP including RL. The environment’s state in non-MDP problems can be experienced via sensor observations and the agent will build its own state estimate internally</em></p>
<p>At the beginning of each episode, the environment and the agent are reset (lines 3–4). On reset, the environment produces an initial state. Then they begin interacting—an agent produces an action given a state (line 6), then the environment produces the next state and reward given the action (line 7), stepping into the next time step. The <code class="docutils literal notranslate"><span class="pre">agent.act-env.step</span></code> cycle continues until the maximum time step <span class="math notranslate nohighlight">\(T\)</span> is reached or the environment terminates. Here we also see a new component, <code class="docutils literal notranslate"><span class="pre">agent.update</span></code> (line 8), which encapsulates an agent’s learning algorithm. Over multiple time steps and episodes, this method collects data and performs learning internally to maximize the objective.</p>
<p>The four foundational ingredients of MDP are:</p>
<ol class="simple">
<li><p>Policy,</p></li>
<li><p>Reward,</p></li>
<li><p>Value function and</p></li>
<li><p>Model of the environment (optionally).</p></li>
</ol>
<p>These are obtained from the <em>dynamics</em> of the <em>finite</em> MDP process.</p>
<div class="math notranslate nohighlight">
\[p(s', r | s , a) = \Pr\{ S_{t+1} = s', R_{t+1} = r | S_{t}=s, A_{t}=a \}\]</div>
<p>where <span class="math notranslate nohighlight">\(s^\prime\)</span> simply translates in English to the successor state whatever the new state is.</p>
<p>The dynamics probability density function maps <span class="math notranslate nohighlight">\(\mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,1]\)</span> and by marginalizing over the appropriate random variables we can get the following distributions.</p>
</section>
<section id="state-transition">
<h3>State transition<a class="headerlink" href="#state-transition" title="Permalink to this headline">#</a></h3>
<p>The action that the agent takes change the environment state to some other state. This can be represented via the environment <em>state transition</em> probabilistic model that generically can be written as:</p>
<div class="math notranslate nohighlight">
\[ p(s'|s,a) = p[S_{t+1}=s^\prime | S_t=s, A_t=a ] = \sum_{r \in \mathcal{R}} p(s', r | s , a)\]</div>
<p>This function can be represented as a state transition probability tensor <span class="math notranslate nohighlight">\(\mathcal P\)</span></p>
<div class="math notranslate nohighlight">
\[\mathcal P^a_{ss^\prime} = p[S_{t+1}=s^\prime | S_t=s, A_t=a ]\]</div>
<p>where one dimension represents the action space and the other two constitute a state transition probability matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>Example</strong>:</p>
<p>Can you determine the state transition tensor for the 4x3 Gridworld ?</p>
</div>
</section>
<section id="reward-function-and-returns">
<h3>Reward function and Returns<a class="headerlink" href="#reward-function-and-returns" title="Permalink to this headline">#</a></h3>
<p>The action will also cause the environment to send the agent a signal called <em>instantaneous reward</em> <span class="math notranslate nohighlight">\(R_{t+1}\)</span>. The reward signal is effectively defining the goal of the agent and is the primary basis for altering a policy. The agent’s sole objective is to maximize the cumulative reward in the long run.</p>
<p>Please note that in the literature the reward is also denoted as <span class="math notranslate nohighlight">\(R_{t}\)</span> - this is a convention issue rather than something fundamental. The justification of the index <span class="math notranslate nohighlight">\(t+1\)</span> is that the environment will take one step to respond to what it receives from the agent.</p>
<p>Another marginalization of the MDP dynamics allows us to get the  <em>reward function</em> that tells us if we are in state <span class="math notranslate nohighlight">\(S_t=s\)</span>, what reward  <span class="math notranslate nohighlight">\(R_{t+1}\)</span>, in expectation, we get when taking an action <span class="math notranslate nohighlight">\(a\)</span>. It is given by,</p>
<div class="math notranslate nohighlight">
\[r(s,a) = \mathop{\mathbb{E}}[ R_{t+1} | S_t=s, A_t=a] = \sum_{r \in \mathcal{R}} r  \sum_{s \in \mathcal{S}}  p(s', r | s , a) \]</div>
<p>This can be written as a matrix <span class="math notranslate nohighlight">\(\mathcal{R}^a_s\)</span>.</p>
<!-- We can also marginalize only with respect to rewards and get the tensor,

$$r(s,a, s') = \mathop{\mathbb{E}}[ R_{t+1} | S_t=s, A_t=a, S_t+1}=s'] = \sum_{r \in \mathcal{R}} r \sum_{s \in \mathcal{S}} p(s', r | s , a)  $$  -->
<section id="returns">
<h4>Returns<a class="headerlink" href="#returns" title="Permalink to this headline">#</a></h4>
<p>To capture the objective,  consider first the <em>return</em> defined as a function of the reward sequence after time step <span class="math notranslate nohighlight">\(t\)</span>. Note that the return is also called <em>utility</em> in some texts.  In the simplest case this function is the total discounted reward,</p>
<div class="math notranslate nohighlight">
\[G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^∞\gamma^k R_{t+1+k}\]</div>
<p>The discount rate determines the present value of future rewards:  a reward received <span class="math notranslate nohighlight">\(k\)</span> time steps in the future is worth only <span class="math notranslate nohighlight">\(γ^{k−1}\)</span>times what it would be worth if it were received immediately.  If <span class="math notranslate nohighlight">\(γ &lt;1\)</span>, the infinite sum above has a finite value as long as the reward sequence <span class="math notranslate nohighlight">\({R_k}\)</span> is bounded.  If <span class="math notranslate nohighlight">\(γ= 0\)</span>, the agent is “myopic” in being concerned only with maximizing immediate rewards:  its objective in this case is to learn how to choose <span class="math notranslate nohighlight">\(A_t\)</span> so  as  to  maximize  only <span class="math notranslate nohighlight">\(R_{t+1}\)</span>.   If  each  of  the  agent’s  actions  happened  to influence only the immediate reward, not future rewards as well, then a myopic agent could maximize by separately maximizing each immediate reward.  But in general, acting to maximize immediate reward can reduce access to future rewards so that the return is reduced.  As <span class="math notranslate nohighlight">\(γ\)</span> approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted.</p>
<p>Notice the two indices needed for its definition - one is the time step <span class="math notranslate nohighlight">\(t\)</span> that manifests where we are in the trajectory and the second index <span class="math notranslate nohighlight">\(k\)</span> is used to index future rewards up to infinity - this is the case of infinite horizon problems where we are not constrained to optimize the agent behavior within the limits of a finite horizon <span class="math notranslate nohighlight">\(T\)</span>. If the discount factor <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span> and the rewards are bounded (<span class="math notranslate nohighlight">\(|R| &lt; R_{max}\)</span>) then the above sum is <em>finite</em>.</p>
<div class="math notranslate nohighlight">
\[ \sum_{k=0}^∞\gamma^k R_{t+1+k} &lt;  \sum_{k=0}^∞\gamma^k R_{max} = \frac{R_{max}}{1-\gamma}\]</div>
<p>The return is itself a random variable - for each trajectory defined by sampling the policy (strategy) of the agent we get a different return. For the Gridworld of the MDP section:</p>
<div class="math notranslate nohighlight">
\[\tau_1: S_0=s_{11}, S_1 = s_{12},  ... S_T=s_{43} \rightarrow G^{\tau_1}_0 = 5.6\]</div>
<div class="math notranslate nohighlight">
\[\tau_2: S_0=s_{11}, S_1=s_{21}, ... , S_T=s_{43} \rightarrow G^{\tau_2}_0 = 6.9\]</div>
<div class="math notranslate nohighlight">
\[ … \]</div>
<p>Note that these are sample numbers to make the point that the return depends on the specific trajectory.</p>
<p>The following is a useful recursion to remind that successive time steps are <em>related</em> to each other:</p>
<div class="math notranslate nohighlight">
\[G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+ \gamma^3 R_{t+4}\]</div>
<div class="math notranslate nohighlight">
\[ = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3}+ \gamma^2 R_{t+4}) \]</div>
<div class="math notranslate nohighlight">
\[ = R_{t+1} + \gamma G_{t+1}\]</div>
</section>
</section>
<section id="policy-function">
<h3>Policy function<a class="headerlink" href="#policy-function" title="Permalink to this headline">#</a></h3>
<p>The agent’s behavior is expressed via a <strong>policy function</strong> <span class="math notranslate nohighlight">\(\pi\)</span> - that tells the agent what <em>action</em> to take for every possible state. The policy is a function of the state and can be:</p>
<ol class="simple">
<li><p>Deterministic functions of the state the environment is in and by extension, the state that the agent is or believes (think about <em>posterior belief</em>) it is in.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[a = \pi(s)\]</div>
<ol class="simple">
<li><p>Stochastic functions of the state expressed as a conditional probability distribution function (conditional pdf) of actions given the current state:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[a \sim p(A_t=a|S_t=s) = \pi(a|s)\]</div>
<p>The policy is assumed to be stationary i.e. not change with time step <span class="math notranslate nohighlight">\(t\)</span> and it will depend only on the state <span class="math notranslate nohighlight">\(S_t\)</span> i.e. <span class="math notranslate nohighlight">\(A_t=a \sim \pi(.|S_t=s), \forall t &gt; 0\)</span>.</p>
</section>
</section>
<section id="value-functions">
<h2>Value Functions<a class="headerlink" href="#value-functions" title="Permalink to this headline">#</a></h2>
<p>The value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.   Whereas rewards determine the immediate,  intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow, and the rewards available in those states.</p>
<section id="state-value">
<h3>State value<a class="headerlink" href="#state-value" title="Permalink to this headline">#</a></h3>
<p>The <em>state-value function</em> <span class="math notranslate nohighlight">\(v_\pi(s)\)</span> provides a notion of the long-term value of state <span class="math notranslate nohighlight">\(s\)</span>. It is equivalent to what other literature calls <em>expected utility</em> . It is defined as the expected_ return starting at state <span class="math notranslate nohighlight">\(s\)</span> and following policy <span class="math notranslate nohighlight">\(\pi(a|s)\)</span>,</p>
<div class="math notranslate nohighlight">
\[v_\pi(s) = \mathop{\mathbb{E}_\pi}(G_t | S_t=s)\]</div>
<p>The expectation is obviously due to the fact that <span class="math notranslate nohighlight">\(G_t\)</span> are random variables since the <em>sequence</em> of states of each potential trajectory starting from <span class="math notranslate nohighlight">\(s\)</span> is dictated by the stochastic policy. As an example, assuming that there are just two possible trajectories from state <span class="math notranslate nohighlight">\(s{11}\)</span> whose returns were calculated above, the value function of state <span class="math notranslate nohighlight">\(s_{11}\)</span> will be</p>
<div class="math notranslate nohighlight">
\[v_\pi(s_{11}) = \frac{1}{2}(G^{\tau_1}_0 + G^{\tau_2}_0)\]</div>
<p>One corner case is interesting - if we make <span class="math notranslate nohighlight">\(\gamma=0\)</span> then <span class="math notranslate nohighlight">\(v_\pi(s)\)</span>  becomes the average of instantaneous rewards we can get from that state.</p>
</section>
<section id="action-value">
<h3>Action value<a class="headerlink" href="#action-value" title="Permalink to this headline">#</a></h3>
<p>We also define the <em>action-value function</em> <span class="math notranslate nohighlight">\(q_\pi(s,a)\)</span> as the expected return starting from the state <span class="math notranslate nohighlight">\(s\)</span>, taking action <span class="math notranslate nohighlight">\(a\)</span> and following policy <span class="math notranslate nohighlight">\(\pi(a|s)\)</span>.</p>
<div class="math notranslate nohighlight">
\[q_\pi(s,a) = \mathop{\mathbb{E}_\pi} (G_t | S_t=s, A_t=a)\]</div>
<p>This is an important quantity as it helps us decide the action we need to take while in state <span class="math notranslate nohighlight">\(s\)</span>.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./aiml-common/lectures/mdp/mdp-intro"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../_index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Markov Decision Processes</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../bellman-expectation-backup/_index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bellman Expectation Backup</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Pantelis Monogioudis, Ph.D<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>