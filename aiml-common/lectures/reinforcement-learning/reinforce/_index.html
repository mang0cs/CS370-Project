
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>The REINFORCE Algorithm &#8212; Introduction to Artificial Intelligence</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-examples.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://pantelis.github.io/artificial-intelligence/aiml-common/lectures/reinforcement-learning/reinforce/_index.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Introduction to Recurrent Neural Networks (RNN)" href="../../rnn/introduction/_index.html" />
    <link rel="prev" title="The SARSA Algorithm" href="../control/sarsa/_index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Artificial Intelligence</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../syllabus/_index.html">
   Syllabus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to AI
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/course-introduction/_index.html">
   Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/data-science-360/_index.html">
   Data Science 360
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/systems-approach/_index.html">
   The four approaches towards AI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/agents/_index.html">
   Agent-Environment Interface
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ai-intro/pipelines/_index.html">
   Pipelines
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../uber-ml-arch-case-study/_index.html">
     A Case Study of an ML Architecture - Uber
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ai-intro/pipelines/02_end_to_end_machine_learning_project.html">
     Setup
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Learning Problem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../learning-problem/_index.html">
   The Learning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../regression/linear-regression/_index.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/sgd/_index.html">
   Stochastic Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../entropy/_index.html">
   Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/maximum-likelihood/_index.html">
   Maximum Likelihood (ML) Estimation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../pgm/bayesian-inference/_index.html">
   Bayesian Inference
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../pgm/bayesian-coin/_index.html">
     Bayesian Coin Flipping
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/classification-intro/_index.html">
   Introduction to Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/logistic-regression/_index.html">
   Logistic Regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Deep Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/perceptron/_index.html">
   The Neuron (Perceptron)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/dnn-intro/_index.html">
   Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-intro/_index.html">
   Introduction to Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-dnn/_index.html">
   Backpropagation in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/regularization/_index.html">
   Regularization in Deep Neural Networks
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Convolutional Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-intro/_index.html">
   Introduction to Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-layers/_index.html">
   CNN Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-example-architectures/_index.html">
   CNN Example Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-example-architectures/using_convnets_with_small_datasets.html">
   Using convnets with small datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-explainers/_index.html">
   CNN Explainers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../transfer-learning/transfer-learning-introduction.html">
   Introduction to Transfer Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../transfer-learning/transfer_learning_tutorial.html">
   Transfer Learning for Computer Vision Tutorial
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Scene Understanding
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/scene-understanding-intro/_index.html">
   Introduction to Scene Understanding
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/feature-extraction-resnet/_index.html">
   Feature Extraction via Residual Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/object-detection/_index.html">
   Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/detection-segmentation-metrics/_index.html">
   Object Detection and Semantic Segmentation Metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/rcnn/_index.html">
   Region-CNN (RCNN) Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/faster-rcnn/_index.html">
   Faster RCNN Object Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../scene-understanding/object-detection/detection-segmentation-workshop/_index.html">
   Object Detection and Semantic Segmentation Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probabilistic Reasoning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/pgm-intro/_index.html">
   Introduction to Probabilistic Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/recursive-state-estimation/_index.html">
   Recursive State Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/bayesian-filter-workshop/_index.html">
   Bayesian Filter Workshop
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/hmm-localization/_index.html">
   Localization and Tracking
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/kalman-workshop/_index.html">
   Kalman Filter Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Logical Reasoning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../logical-reasoning/automated-reasoning/_index.html">
   Automated Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../logical-reasoning/propositional-logic/_index.html">
   World Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../logical-reasoning/logical-agents/_index.html">
   Logical Agents
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Planning without Interactions
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/_index.html">
   Planning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../autonomous-cars/_index.html">
   Autonomous Agents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../autonomous-cars/imitation-learning/_index.html">
   Imitation Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/classical-planning/_index.html">
   Classical Planning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/search/_index.html">
   Planning with Search
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/search/forward-search/_index.html">
   Forward Search Algorithms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../planning/search/a-star/_index.html">
   The A* Algorithm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Markov Decision Processes
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/_index.html">
   Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-intro/_index.html">
   Introduction to MDP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/bellman-expectation-backup/_index.html">
   Bellman Expectation Backup
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/bellman-optimality-backup/_index.html">
   Bellman Optimality Backup
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dynamic Programming Algorithms
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-dp-algorithms/policy-iteration/_index.html">
   Dynamic Programming Algorithms - Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-dp-algorithms/policy-evaluation/_index.html">
   Policy Evaluation (Prediction)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-dp-algorithms/policy-improvement/_index.html">
   Policy Improvement (Control)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-dp-algorithms/value-iteration/_index.html">
   Dynamic Programming Algorithms - Value Iteration
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  MDP Lab
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-lab/recycling-robot/_index.html">
   Finding the optimal policy of a recycling robot.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../mdp/mdp-lab/yield-management-capacity-control/_index.html">
   Optimal Capacity Control
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../_index.html">
   Reinforcement Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prediction/monte-carlo.html">
   Monte-Carlo Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../prediction/temporal-difference.html">
   Temporal Difference (TD) Prediction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../control/_index.html">
   Model-free Control
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../generalized-policy-iteration/_index.html">
   Generalized Policy Iteration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../control/sarsa/_index.html">
   The SARSA Algorithm
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   The REINFORCE Algorithm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Sequences and RNNs
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/introduction/_index.html">
   Introduction to Recurrent Neural Networks (RNN)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/simple-rnn/_index.html">
   Simple RNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/15_processing_sequences_using_rnns_and_cnns.html">
   Processing Sequences Using RNN
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../rnn/lstm/_index.html">
   The Long Short-Term Memory (LSTM) Cell Architecture
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural Language Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nlp-intro/_index.html">
   Introduction to NLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/word2vec/_index.html">
   Word2Vec Embeddings
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/rnn-language-models/_index.html">
   RNN Language Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nmt/_index.html">
   Neural Machine Translation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/nmt-metrics/_index.html">
   NMT Metrics - BLEU
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/attention/_index.html">
   Attention in NMT
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../nlp/transformers/_index.html">
   Transformers Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../ml-math/_index.html">
   Math for ML
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-math/probability/_index.html">
     Probability Basics
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../linear-algebra/_index.html">
     Linear Algebra for Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../ml-math/calculus/_index.html">
     Calculus
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../resources/environment/_index.html">
   Your Programming Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../python/_index.html">
   Learn Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../runbook.html">
   Executable Content Runbook
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments &amp; Projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../assignments/probability/probability-assignment-3/index.html">
   Probability Assignment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../assignments/bayesian-learning/index.html">
   Bayesian Learning Agents
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../projects/usv/_index.html">
   Unmanned Surface Vehicle Navigation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../projects/vqa/index.html">
   Separating Perception and Reasoning in VQA
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pantelis/artificial-intelligence"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pantelis/artificial-intelligence/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/reinforcement-learning/reinforce/_index.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../../_sources/aiml-common/lectures/reinforcement-learning/reinforce/_index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-network">
   Policy Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-reinforce-algorithm">
   Applying the REINFORCE algorithm
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The REINFORCE Algorithm</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-network">
   Policy Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-reinforce-algorithm">
   Applying the REINFORCE algorithm
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="the-reinforce-algorithm">
<h1>The REINFORCE Algorithm<a class="headerlink" href="#the-reinforce-algorithm" title="Permalink to this headline">#</a></h1>
<p>Given that RL can be posed as an MDP, in this section we continue with a policy-based algorithm that learns the policy <em>directly</em> by optimizing the objective function and can then map the states to actions.  The algorithm we treat here, called REINFORCE, is important although more modern algorithms do perform better.</p>
<p>It took its name from the fact that during <em>training</em> actions that resulted in good outcomes should become more probable—these actions are positively <em>reinforced</em>. Conversely, actions which resulted in bad outcomes should become less probable. If learning is successful, over the course of many iterations, action probabilities produced by the policy, shift to a distribution that results in good performance in an environment. Action probabilities are changed by following the policy gradient, therefore REINFORCE is known as a <em>policy gradient</em> algorithm.</p>
<p>The algorithm needs three components:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Parametrized policy <span class="math notranslate nohighlight">\(\pi_\theta (a|s)\)</span></p></td>
<td><p>The key idea of the algorithm is to learn a good policy, and this means doing function approximation. Neural networks are powerful and flexible function approximators, so we can represent a policy using a deep neural network (DNN) consisting of learnable parameters <span class="math notranslate nohighlight">\(\mathbf \theta\)</span>. This is often referred to as a policy network <span class="math notranslate nohighlight">\(\pi_θ\)</span>. We say that the policy is parametrized by  <span class="math notranslate nohighlight">\(\theta\)</span>. Each specific set of values of the parameters of the policy network represents a particular policy. To see why, consider <span class="math notranslate nohighlight">\(θ_1 ≠ θ_2\)</span>. For any given state <span class="math notranslate nohighlight">\(s\)</span>, different policy networks may output different sets of action probabilities, that is, <span class="math notranslate nohighlight">\(\pi_{θ_1}(a | s) \neq \pi_{θ_2}(a | s)\)</span>. The mappings from states to action probabilities are different so we say that <span class="math notranslate nohighlight">\(π_{θ_1}\)</span> and <span class="math notranslate nohighlight">\(π_{θ_2}\)</span> are different policies. A single DNN is therefore capable of representing many different policies.</p></td>
</tr>
<tr class="row-odd"><td><p>The objective to be maximized <span class="math notranslate nohighlight">\(J(\pi_\theta)\)</span><a class="footnote-reference brackets" href="#id3" id="id1">1</a></p></td>
<td><p>At this point is nothing else other than the expected discounted <em>return</em> over policy, just like in MDP.</p></td>
</tr>
<tr class="row-even"><td><p>Policy Gradient</p></td>
<td><p>A method for updating the policy parameters <span class="math notranslate nohighlight">\(\theta\)</span>. The policy gradient algorithm searches for a local maximum in <span class="math notranslate nohighlight">\(J(\pi_\theta)\)</span>:  <span class="math notranslate nohighlight">\(\max_\theta J(\pi_\theta)\)</span>. This is the common gradient ascent algorithm that we met in a similar form in neural networks. $<span class="math notranslate nohighlight">\(\theta ← \theta + \alpha \nabla_\theta J(\pi_\theta)\)</span><span class="math notranslate nohighlight">\( where \)</span>\alpha$ is the learning rate.</p></td>
</tr>
</tbody>
</table>
<p>Out of the three components, the most complicated one is the policy gradient that can be shown to be given by the differentiable quantity:</p>
<div class="math notranslate nohighlight">
\[ \nabla_\theta J(\pi_\theta)= \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (a|s) v_\pi (s) \right ]\]</div>
<p>We understand that this expression came out of nowhere but the interested reader can find its detailed derivation in the chapter 2 of <a class="reference external" href="https://www.amazon.com/Deep-Reinforcement-Learning-Python-Hands/dp/0135172381">this</a> reference. We can approximate the value at state <span class="math notranslate nohighlight">\(s\)</span> with the return over many sample trajectories <span class="math notranslate nohighlight">\(\tau\)</span> that are sampled from the policy network.</p>
<div class="math notranslate nohighlight">
\[ \nabla_\theta J(\pi_\theta)= \mathbb{E}_{\tau \sim \pi_\theta} \left[ G_t \nabla_\theta \log \pi_\theta (a|s) \right ]\]</div>
<p>where <span class="math notranslate nohighlight">\(G_t\)</span> is the <em>return</em> - a quantity we have seen earlier albeit now the return is limited by the length of each trajectory just like in MC method,</p>
<div class="math notranslate nohighlight">
\[G_t(\tau) = \sum_{k=0}^{T-1}\gamma^k R_{t+1+k}\]</div>
<p>The <span class="math notranslate nohighlight">\(\gamma\)</span> is usually a hyper-parameter that we need to optimize usually iterating over many values in [0.01,…,0.99] and selecting the one with the best results.</p>
<p>We also have an expectation in the gradient expression that we need to address.  The expectation <span class="math notranslate nohighlight">\(\mathbb E_{\tau \sim \pi_\theta}\)</span> we need to take is approximated with a summation over <em>each</em> trajectory aka a Monte-Carlo approximation. Effectively, we are generating the right hand side as in line 8 in the code below, by sampling a trajectory (line 4) and estimating its return (line 7) in a completely model-free fashion i.e. without assuming any knowledge of the transition and reward functions. This is implemented next:</p>
<p>1: Initialize learning rate <span class="math notranslate nohighlight">\(\alpha\)</span></p>
<p>2: Initialize weights θ of a policy network <span class="math notranslate nohighlight">\(π_θ\)</span></p>
<p>3: for episode = 0, … , MAX_EPISODE do</p>
<p>4:   Sample a trajectory using the policy network <span class="math notranslate nohighlight">\(\pi_\theta\)</span>, <span class="math notranslate nohighlight">\(τ = s_0, a_0, r_0, . . . , s_T, a_T, r_T\)</span></p>
<p>5:   Set <span class="math notranslate nohighlight">\(∇_θ J(π_θ) = 0\)</span></p>
<p>6:    for t = 0, … , T-1 do</p>
<p>7:               Calculate <span class="math notranslate nohighlight">\(G_t(τ)\)</span></p>
<p>8:               <span class="math notranslate nohighlight">\(\nabla_\theta J(\pi_\theta) = \nabla_\theta J(\pi_\theta) + G_t (τ) \nabla_\theta \log \pi_\theta (a_t|s_t) \)</span></p>
<p>9:        end for</p>
<p>10:        <span class="math notranslate nohighlight">\(θ = θ + α ∇_θ J(π_θ)\)</span></p>
<p>11: end for</p>
<p>It is important that a trajectory is discarded after each parameter update—it cannot be reused. This is because REINFORCE is an <em>on-policy</em> algorithm just like the MC it “learns on the job”. This is evidently seen in line 10 where the parameter update equation uses the policy gradient that itself (line 8) directly depends on action probabilities <span class="math notranslate nohighlight">\(π_θ(a_t | s_t)\)</span> generated by the <em>current</em> policy <span class="math notranslate nohighlight">\(π_θ\)</span> only and not some past policy <span class="math notranslate nohighlight">\(π_{θ′}\)</span>. Correspondingly, the return <span class="math notranslate nohighlight">\(G_t(τ)\)</span> where <span class="math notranslate nohighlight">\(τ ~ π_θ\)</span> must also be generated from <span class="math notranslate nohighlight">\(π_θ\)</span>, otherwise the action probabilities will be adjusted based on returns that the policy wouldn’t have generated.</p>
<section id="policy-network">
<h2>Policy Network<a class="headerlink" href="#policy-network" title="Permalink to this headline">#</a></h2>
<p>One of the key ingredients that REINFORCE introduces is the policy network that is approximated with a DNN eg. a fully connected neural network with a number of hidden layers that is hyper-parameter (e.g. 2 RELU layers).</p>
<p>1: Given a policy network <code class="docutils literal notranslate"><span class="pre">net</span></code>, a <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> (multinomial) distribution class, and a <code class="docutils literal notranslate"><span class="pre">state</span></code></p>
<p>2: Compute the output <code class="docutils literal notranslate"><span class="pre">pdparams</span> <span class="pre">=</span> <span class="pre">net(state)</span></code></p>
<p>3: Construct an instance of an action probability distribution  <code class="docutils literal notranslate"><span class="pre">pd</span> <span class="pre">=</span> <span class="pre">Categorical(logits=pdparams)</span></code></p>
<p>4: Use pd to sample an action, <code class="docutils literal notranslate"><span class="pre">action</span> <span class="pre">=</span> <span class="pre">pd.sample()</span></code></p>
<p>5: Use pd and action to compute the action log probability, <code class="docutils literal notranslate"><span class="pre">log_prob</span> <span class="pre">=</span> <span class="pre">pd.log_prob(action)</span></code></p>
<p>Other discrete distributions can be used and many actual libraries parametrize continuous distributions such as Gaussians.</p>
</section>
<section id="applying-the-reinforce-algorithm">
<h2>Applying the REINFORCE algorithm<a class="headerlink" href="#applying-the-reinforce-algorithm" title="Permalink to this headline">#</a></h2>
<p>It is now instructive to see an stand-alone example in python for the so called <code class="docutils literal notranslate"><span class="pre">CartPole-v0</span></code> <a class="footnote-reference brackets" href="#id4" id="id2">2</a></p>
<p><img alt="cartpole" src="../../../../_images/cartpole.png" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="mi">1</span>  <span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
 <span class="mi">2</span>  <span class="kn">import</span> <span class="nn">gym</span>
 <span class="mi">3</span>  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
 <span class="mi">4</span>  <span class="kn">import</span> <span class="nn">torch</span>
 <span class="mi">5</span>  <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
 <span class="mi">6</span>  <span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
 <span class="mi">7</span>
 <span class="mi">8</span>  <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
 <span class="mi">9</span>
<span class="mi">10</span>  <span class="k">class</span> <span class="nc">Pi</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="mi">11</span>      <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
<span class="mi">12</span>          <span class="nb">super</span><span class="p">(</span><span class="n">Pi</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="mi">13</span>          <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
<span class="mi">14</span>              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
<span class="mi">15</span>              <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
<span class="mi">16</span>              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span>
<span class="mi">17</span>          <span class="p">]</span>
<span class="mi">18</span>          <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
<span class="mi">19</span>          <span class="bp">self</span><span class="o">.</span><span class="n">onpolicy_reset</span><span class="p">()</span>
<span class="mi">20</span>          <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># set training mode</span>
<span class="mi">21</span>
<span class="mi">22</span>      <span class="k">def</span> <span class="nf">onpolicy_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="mi">23</span>          <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="mi">24</span>          <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="mi">25</span>
<span class="mi">26</span>      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="mi">27</span>          <span class="n">pdparam</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="mi">28</span>          <span class="k">return</span> <span class="n">pdparam</span>
<span class="mi">29</span>
<span class="mi">30</span>      <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="mi">31</span>          <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span> <span class="c1"># to tensor</span>
<span class="mi">32</span>          <span class="n">pdparam</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># forward pass</span>
<span class="mi">33</span>          <span class="n">pd</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">pdparam</span><span class="p">)</span> <span class="c1"># probability distribution</span>
<span class="mi">34</span>          <span class="n">action</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># pi(a|s) in action via pd</span>
<span class="mi">35</span>          <span class="n">log_prob</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># log_prob of pi(a|s)</span>
<span class="mi">36</span>          <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span> <span class="c1"># store for training</span>
<span class="mi">37</span>          <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="mi">38</span>
<span class="mi">39</span>  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="mi">40</span>      <span class="c1"># Inner gradient-ascent loop of REINFORCE algorithm</span>
<span class="mi">41</span>      <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
<span class="mi">42</span>      <span class="n">rets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># the returns</span>
<span class="mi">43</span>      <span class="n">future_ret</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="mi">44</span>      <span class="c1"># compute the returns efficiently</span>
<span class="mi">45</span>      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
<span class="mi">46</span>          <span class="n">future_ret</span> <span class="o">=</span> <span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">future_ret</span>
<span class="mi">47</span>          <span class="n">rets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">future_ret</span>
<span class="mi">48</span>      <span class="n">rets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rets</span><span class="p">)</span>
<span class="mi">49</span>      <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">log_probs</span><span class="p">)</span>
<span class="mi">50</span>      <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">log_probs</span> <span class="o">*</span> <span class="n">rets</span> <span class="c1"># gradient term; Negative for maximizing</span>
<span class="mi">51</span>      <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="mi">52</span>      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="mi">53</span>      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># backpropagate, compute gradients</span>
<span class="mi">54</span>      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># gradient-ascent, update the weights</span>
<span class="mi">55</span>      <span class="k">return</span> <span class="n">loss</span>
<span class="mi">56</span>
<span class="mi">57</span>  <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="mi">58</span>      <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">)</span>
<span class="mi">59</span>      <span class="n">in_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 4</span>
<span class="mi">60</span>      <span class="n">out_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span> <span class="c1"># 2</span>
<span class="mi">61</span>      <span class="n">pi</span> <span class="o">=</span> <span class="n">Pi</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="c1"># policy pi_theta for REINFORCE</span>
<span class="mi">62</span>      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="mi">63</span>      <span class="k">for</span> <span class="n">epi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
<span class="mi">64</span>          <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="mi">65</span>          <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span> <span class="c1"># cartpole max timestep is 200</span>
<span class="mi">66</span>              <span class="n">action</span> <span class="o">=</span> <span class="n">pi</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="mi">67</span>              <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="mi">68</span>              <span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
<span class="mi">69</span>              <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
<span class="mi">70</span>              <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
<span class="mi">71</span>                  <span class="k">break</span>
<span class="mi">72</span>          <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="c1"># train per episode</span>
<span class="mi">73</span>          <span class="n">total_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
<span class="mi">74</span>          <span class="n">solved</span> <span class="o">=</span> <span class="n">total_reward</span> <span class="o">&gt;</span> <span class="mf">195.0</span>
<span class="mi">75</span>          <span class="n">pi</span><span class="o">.</span><span class="n">onpolicy_reset</span><span class="p">()</span> <span class="c1"># onpolicy: clear memory after training</span>
<span class="mi">76</span>          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Episode </span><span class="si">{</span><span class="n">epi</span><span class="si">}</span><span class="s1">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">, </span><span class="se">\</span>
<span class="s1">77          total_reward: </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s1">, solved: </span><span class="si">{</span><span class="n">solved</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="mi">78</span>
<span class="mi">79</span>  <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="mi">80</span>      <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>The REINFORCE algorithm presented here can generally be applied to continuous and discreet problems but it has been shown to possess high variance and sample-inefficiency. Several improvements have been proposed and the interested reader can refer to section 2.5.1 of the suggested book.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Notation wise, since we need to have a bit more flexibility in RL problems, we will use the symbol <span class="math notranslate nohighlight">\(J(\pi_\theta)\)</span> as the objective function.</p>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Please note that SLM-Lab, the library that accompanies the suggested in the syllabus book, is a mature library and probably a good example of how to develop ML/RL libraries in python. You will learn a lot by reviewing the implementations under the <code class="docutils literal notranslate"><span class="pre">agents/algorithms</span></code> directory to get a feel of how RL problems are abstracted .</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./aiml-common/lectures/reinforcement-learning/reinforce"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../control/sarsa/_index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">The SARSA Algorithm</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../../rnn/introduction/_index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to Recurrent Neural Networks (RNN)</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Pantelis Monogioudis, Ph.D<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>