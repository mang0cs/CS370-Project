{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pantelis-nlp/tutorial-nlp-notebooks/blob/main/rnn_language_model.ipynb)\n",
    "\n",
    "# CNN Language Model\n",
    "\n",
    "The following was developed by Harini Appansrinivasan, NYU as part of an assignment submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import array\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers import Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "# split input sequence into inputs X of size = n_steps and targets Y with the subsequent integer after each X\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\tend_ix = i + n_steps                                     # find the end of this pattern\n",
    "\t\tif end_ix > len(sequence)-1:                             # condition to check if we are beyond the input sequence\n",
    "\t\t\tbreak\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]      # gather input and output parts of the pattern\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "# creating a vocabulary of unique characters\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "# creating a dictionary, mapping characters to index and index to characters\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "print(char_to_ix)\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "print(ix_to_char)\n",
    "seq_length = 25        # number of characters in each input sequence from which the next char is predicted\n",
    "data_tokens = [char_to_ix[ch] for ch in list(data)]               # convert all characters in data string into t tokens\n",
    "X_tokens, y_tokens = split_sequence(data_tokens, seq_length)      # split into samples, inputs X and targets y\n",
    "for i in range(len(X_tokens)):   \n",
    "\tprint(X_tokens[i], y_tokens[i])                                 # summarize the split input and target tokens\n",
    "num_samples = X_tokens.shape[0]\n",
    "print(X_tokens.shape)     # shape of tokenized input array with sequence length = 25\n",
    "print(y_tokens.shape)     # shape of tokenized target array\n",
    "print(num_samples)        # total number of inputs to the CNN\n",
    "print(seq_length)         # number of characters in each input sequence from which the next char is predicted\n",
    "print(vocab_size)         # total number of unique characters, ie, number of output classes\n",
    "# function to one-hot encode the data for each unique class k=vocab_size\n",
    "def onehot_encoding(data):\n",
    "  if (len(data.shape)==2):\n",
    "    onehot_data = np.zeros((data.shape[0] * data.shape[1], data.max()+1), dtype=int)   # a 3D array of 0's\n",
    "    onehot_data[np.arange(data.shape[0] * data.shape[1]), data.flatten()] = 1          # replace 0 with 1 at that index of the original array\n",
    "    onehot_data = onehot_data.reshape(data.shape[0], data.shape[1], vocab_size)        # reshape into [num_samples, seq_length, vocab_size] format\n",
    "  elif (len(data.shape)==1):\n",
    "    onehot_data = np.zeros((data.shape[0], data.max()+1), dtype=int)                   # a 2D array of 0's\n",
    "    onehot_data[np.arange(data.shape[0]), data.flatten()] = 1                          # replace 0 with 1 at that index of the original array\n",
    "    onehot_data = onehot_data.reshape(data.shape[0], vocab_size)                       # reshape into [num_samples, vocab_size] format\n",
    "  return onehot_data\n",
    "X_onehot = onehot_encoding(X_tokens)  # one-hot encode the inputs\n",
    "X_onehot.shape                        # shape = [num_samples, seq_length, vocab_size]\n",
    "y_onehot = onehot_encoding(y_tokens)  # one-hot encode the targets\n",
    "y_onehot.shape                        # shape = [num_samples, vocab_size]\n",
    "# reset model\n",
    "tf.keras.backend.clear_session\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "# conv1d layer with input dim (seq_length, vocab_size), ReLU activation, 64 7x7 filters\n",
    "model.add(Conv1D(filters=64, kernel_size=7, activation='relu', input_shape=(seq_length, vocab_size)))  \n",
    "\n",
    "# maxpool layer, with pool size=2\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "# flatten the data before passing it to a fully connected / dense layer\n",
    "model.add(Flatten())\n",
    "\n",
    "# a dense layer with output size=43 for the 43 unique classes and a softmax activation to give the 43 class probabilities\n",
    "model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "# compile the model with adam optimizer and cross entropy loss. Print accuracy metrics while training the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "X_input = X_onehot\n",
    "y_target = y_onehot\n",
    "# function to test the model and predict characters after each epoch\n",
    "class PredictionCallback(tf.keras.callbacks.Callback):    \n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    txt = ''\n",
    "   \n",
    "    x = X_input[0:1,:,:]                          # set the first input as the first 25 characters\n",
    "    for i in range(seq_length):                   # convert the one_hot encoded x inputs to tokens and then to characters, add them to string 'txt'\n",
    "      txt = ''.join([txt,ix_to_char[np.argmax(x[0,i,:])]])\n",
    "\n",
    "    n = 0                                         # data pointer\n",
    "    while(n < num_samples):                       # run the prediction loop until the end of total number of inputs to the CNN\n",
    "      n = n + 1            \n",
    "      y_pred = self.model.predict(x, verbose=0)   # predict the class probabilities for the next character from a sequence of 25 encoded characters\n",
    "      ix = np.argmax(y_pred)                      # find the index of the value with the largest probability\n",
    "      txt = ''.join([txt,ix_to_char[ix]])         # convert the prediction into character and add it to the list of previous predictions \n",
    "\n",
    "      # at test-time, feed back the predicted character to model for next character prediction\n",
    "      ypred_onehot = np.zeros((1,vocab_size))     # one-hot encode the predicted probabilities\n",
    "      ypred_onehot[:,ix] = 1\n",
    "\n",
    "      x = x.reshape(seq_length,vocab_size)        # reshape x into [seq_length, vocab_size] before stacking y_pred\n",
    "      x = np.vstack([x[1:,:],ypred_onehot])       # remove the first character from x and stack y_pred for the next iteration\n",
    "      x = x.astype(int)                           # numpy vstack returns a float array, convert it into an integer array\n",
    "      x = x.reshape(1,seq_length,vocab_size)      # reshape the input x into [1, seq_length, vocab_size]\n",
    "\n",
    "    print('----\\n %s \\n----' % (txt, ))           # print the entire string of our model predictions\n",
    "# train the model for 100 epochs on X_input\n",
    "# call the PredictionCallback() function to predict all characters and print them after each epoch\n",
    "# print the loss and accuracies after each epoch\n",
    "hist = model.fit(X_input, y_target, epochs=100, verbose=2, callbacks=[PredictionCallback()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
