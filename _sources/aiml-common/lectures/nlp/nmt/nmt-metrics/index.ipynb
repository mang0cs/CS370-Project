{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMT Metrics  - BLEU\n",
    "\n",
    "In 2002, IBM researchers developed the [BiLingual Evaluation Understudy (BLEU)](https://aclanthology.org/P02-1040/) metric that remains, with its many variants to this day, one of the most quoted [metrics](https://huggingface.co/evaluate-metric) for evaluating machine translation applications. \n",
    "\n",
    "## Intuition\n",
    "\n",
    "The BLEU algorithm evaluates the precision score of a candidate machine translation against a reference human translation. \n",
    "\n",
    "The reference human translation is a assumed to be a model example of a translation, and we use _n-gram_ matches as our metric for how similar a candidate translation is to it. Why simple precision can't be used? Consider the following example of poor machine translation output with high precision metric using unigrams. \n",
    "\n",
    "| Output      |       |     |     |     |     |   |\n",
    "| ----------- | ----- | --- | --- | --- | --- | --- |\n",
    "| Candidate   | the   | the | the | the | the | the | the |\n",
    "| Reference 1 | the   | cat | is  | on  | the | mat |\n",
    "| Reference 2 | there | is  | a   | cat | on  | the | mat |\n",
    "\n",
    "Of the seven words in the candidate translation, all of them appear in the reference translations. Thus the candidate text is given a unigram precision of,\n",
    "\n",
    "$$ P= \\frac{m}{w_{t}} = \\frac{7}{7}=1$$\n",
    "\n",
    "where $m$ is number of words from the candidate that are found in the reference, and $w_t$ is the total number of words in the candidate. This is a perfect score, despite the fact that the candidate translation above retains little of the content of either of the references.\n",
    "\n",
    "The modification that BLEU makes is fairly straightforward. For each word in the candidate translation, the algorithm takes its maximum total count, $m_{max}$, in any of the reference translations. In the example above, the word \"the\" appears twice in reference 1, and once in reference 2. Thus $m_{max}=2$.\n",
    "\n",
    "For the candidate translation, the count $m_{w}$ of each word is clipped to a maximum of $m_{max}$ for that word. In this case, \"the\" has $m_w=7$ and $m_{max}=2$ thus $m_w$ is clipped to 2. These clipped counts $m_w$ are then summed over all distinct words in the candidate. This sum is then divided by the total number of unigrams in the candidate translation. In the above example, the modified unigram precision score would be:\n",
    "\n",
    "$$P= \\frac{2}{7}$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, the BLUE score is a cumulative score that refers to the calculation of individual n-gram scores at all orders from 1 to n and weighs them by calculating the weighted _geometric mean_, i.e.\n",
    "\n",
    "$$BLEU = \\beta \\prod_{i=1}^k p_n^{w_n}$$\n",
    "\n",
    "where the brevity penalty $\\beta$ considers the length of the reference (R) and candidate (C) sentences. \n",
    "\n",
    "$$\\beta = \\exp(\\min(0, 1- \\frac{len(R)}{len(C)}))$$\n",
    "\n",
    "and $p_n$ is the n-gram precision. \n",
    "\n",
    "You can remember BLUE by drawing the diagram we saw in binary classification metrics  \n",
    "\n",
    "![precision-recall](images/precision-recall.png)\n",
    "\n",
    "and make the following associations\n",
    "\n",
    "| Metric | Description as it applies to the NMT         |\n",
    "| ------ | -------------------------------------------- |\n",
    "| TP     | There are n-grams in C that are in R         |\n",
    "| FP     | There are n-grams in C that are not in R     |\n",
    "| FN     | There are missing n-grams in C that are in R |\n",
    "\n",
    "BLUE is concerned with the first two rows and calculates the n-gram precision as the usual ratio of the counts $\\frac{TP}{TP+FP}$.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation\n",
    "\n",
    "A simple practical example is shown next. Note that the weights are specified as a tuple where each index refers to the gram order. To calculate the BLEU score only for 1-gram matches, you can specify a weight of 1 for 1-gram and 0 for 2, 3 and 4 (1, 0, 0, 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# 1-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "print(score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, however, using individual words as the unit of comparison is not optimal. Instead, BLEU computes the same modified precision metric using n-grams. The length which has the \"highest correlation with monolingual human judgement\" was found to be **four**. This is also the default in many packages that calculate BLUE scores (BLUE-4 metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 1.000000\n",
      "Individual 2-gram: 1.000000\n",
      "Individual 3-gram: 1.000000\n",
      "Individual 4-gram: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# n-gram individual BLEU\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
